from nltk.tokenize import sent_tokenize, word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet


def process_sentence(Sentence):
    # tokenize sentence
    tokenized_word = word_tokenize(Sentence)

    # stop words
    stop_words = set(stopwords.words("english"))
    filtered_list = []

    for word in tokenized_word:
        if word.casefold() not in stop_words:
            filtered_list.append(word)

    lemmatizer = WordNetLemmatizer()

    pos_tagged = pos_tag(filtered_list)
    print(pos_tagged)

    def get_wordnet_pos(treebank_tag):
        if treebank_tag.startswith('J'):
            return wordnet.ADJ
        elif treebank_tag.startswith('V'):
            return wordnet.VERB
        elif treebank_tag.startswith('N'):
            return wordnet.NOUN
        elif treebank_tag.startswith('R'):
            return wordnet.ADV
        else:
            return None

    lemmatized_words = []
    for word, treebank_tag in pos_tagged:
        wordnet_tag = get_wordnet_pos(treebank_tag)
        lemma = lemmatizer.lemmatize(word, pos=wordnet_tag)
        lemmatized_words.append(lemma)
    print(lemmatized_words)


def Response():
    x = ""


def main():
    Sentence = "what time is it"
    process_sentence(Sentence)


if __name__ == "__main__":
    main()
